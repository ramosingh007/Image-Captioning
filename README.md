# ğŸ–¼ï¸ Image Captioning with Vision-Language LLM

This project uses **Unsloth's Qwen2.5-VL-7B-Instruct** model to generate captions from images â€” designed especially for accessibility use cases like describing images for blind users.

---

## ğŸš€ Features

- Caption any image using a local vision-language model
- Uses the [Unsloth](https://unsloth.ai) library for fast inference
- Outputs stored in `captions.txt`

---

## ğŸ“‚ Folder Structure

image-captioning/
â”‚
â”œâ”€â”€ caption_generator.py # Main script to run
â”œâ”€â”€ requirements.txt # Needed packages
â”œâ”€â”€ .gitignore # Ignore unnecessary files
â”œâ”€â”€ README.md # Project overview
â””â”€â”€ captions.txt # Output log (auto-created)


---

## ğŸ§  How It Works

1. Loads a vision-language model (`Qwen2.5-VL`)
2. Accepts a local image (`class.jpeg`)
3. Prompts the model to describe the image
4. Saves captioned results to `captions.txt`

---

## ğŸ› ï¸ Run It Locally

```bash
# Step 1: Install dependencies
pip install -r requirements.txt

# Step 2: Run the script
python caption_generator.py
